{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108310f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-18T21:20:12.284323Z",
     "start_time": "2021-11-18T21:20:10.025554Z"
    }
   },
   "outputs": [],
   "source": [
    "############ IMPORTS ####################\n",
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.utils.data as dataf\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io as sio\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.nn.parameter import Parameter\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from PIL import Image\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from operator import truediv\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from torchsummary import summary\n",
    "import record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8557e548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-18T21:20:12.310793Z",
     "start_time": "2021-11-18T21:20:12.286615Z"
    }
   },
   "outputs": [],
   "source": [
    "def loadData(name):\n",
    "    \n",
    "    if name == \"Houston\":\n",
    "        dataHSI = sio.loadmat('./../Houston/houston.mat')['houston']\n",
    "\n",
    "        dataLIDAR = np.array(Image.open('./../Houston/houston_lidar.tif'))\n",
    "        dataLIDAR = dataLIDAR.reshape(dataLIDAR.shape[0],dataLIDAR.shape[1],1)\n",
    "\n",
    "        \n",
    "        labels = sio.loadmat('./../Houston/houston_gt.mat')['houston_gt_te']\n",
    "        \n",
    "        labels += sio.loadmat('./../Houston/houston_gt.mat')['houston_gt_tr']\n",
    "\n",
    "        \n",
    "    elif(name == \"Trento\"):\n",
    "        \n",
    "        dataHSI = sio.loadmat('./../Trento/HSI.mat')['HSI']\n",
    "\n",
    "        dataLIDAR = sio.loadmat('./../Trento/LiDAR.mat')['LiDAR']\n",
    "        dataLIDAR = dataLIDAR.reshape(dataLIDAR.shape[0],dataLIDAR.shape[1],1)\n",
    "        \n",
    "        labels = sio.loadmat('./../Trento/TSLabel.mat')['TSLabel']\n",
    "        labels += sio.loadmat('./../Trento/TRLabel.mat')['TRLabel']\n",
    "        \n",
    "    elif(name == \"MUUFL\"):\n",
    "        \n",
    "        dataHSI = sio.loadmat('./../MUUFL/muufl_share.mat')['hsi_img']\n",
    "\n",
    "        dataLIDAR = sio.loadmat('./../MUUFL/muufl_share.mat')['lidarz']\n",
    "        labels = sio.loadmat('./../MUUFL/muufl_share.mat')['labels']\n",
    "    return dataHSI,dataLIDAR, labels\n",
    "\n",
    "\n",
    "def splitTrainTestSet(X, y, testRatio, randomState=345):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState,\n",
    "                                                        stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def applyPCA(X, numComponents=75):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "    return newX, pca\n",
    "\n",
    "def normalizeHSI(X):\n",
    "    for i in range(X.shape[2]):\n",
    "        minimal = X[:, :, i].min()\n",
    "        maximal = X[:, :, i].max()\n",
    "        X[:, :, i] = (X[:, :, i] - minimal)/(maximal - minimal)\n",
    "    return X\n",
    "        \n",
    "def normalizeLIDAR(X):\n",
    "    minimal = X.min()\n",
    "    maximal = X.max()\n",
    "    X = (X - minimal)/(maximal - minimal)\n",
    "    return X\n",
    "\n",
    "def padWithZeros(X, margin=2):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return newX\n",
    "\n",
    "def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n",
    "    margin = int((windowSize - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
    "    print(zeroPaddedX.shape)\n",
    "    # split patches\n",
    "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]), dtype=np.float32)\n",
    "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]), dtype=int)\n",
    "    patchIndex = 0\n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
    "            patchIndex = patchIndex + 1\n",
    "    if removeZeroLabels:\n",
    "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
    "        patchesLabels = patchesLabels[patchesLabels>0]\n",
    "        \n",
    "    return patchesData, patchesLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06163fb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-18T21:31:05.562784Z",
     "start_time": "2021-11-18T21:31:05.553397Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv2d_cd(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
    "                 padding=1, dilation=1, groups=1, bias=False, theta=0.7):\n",
    "\n",
    "        super(Conv2d_cd, self).__init__() \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.theta = theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_normal = self.conv(x)\n",
    "        \n",
    "        if math.fabs(self.theta - 0.0) < 1e-8:\n",
    "            return out_normal \n",
    "        else:\n",
    "            #pdb.set_trace()\n",
    "            [C_out,C_in, kernel_size,kernel_size] = self.conv.weight.shape\n",
    "            \n",
    "            kernel_diff = self.conv.weight.sum(2).sum(2)\n",
    "            kernel_diff = kernel_diff[:, :, None, None]\n",
    "            \n",
    "            out_diff = F.conv2d(input=x, weight=kernel_diff, bias=self.conv.bias, stride=self.conv.stride, padding=0, groups=self.conv.groups)\n",
    "            \n",
    "            return out_normal - self.theta * out_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8ee19f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-18T21:31:06.981506Z",
     "start_time": "2021-11-18T21:31:06.969508Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, FM, Classes, patchsize, NC):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            Conv2d_cd(\n",
    "                in_channels = NC,\n",
    "                out_channels = FM,\n",
    "                kernel_size = (3, 3),\n",
    "                stride = 1,\n",
    "                padding = (1,1)\n",
    "            ),\n",
    "            nn.BatchNorm2d(FM),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "#             nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            Conv2d_cd(FM, FM//2, (3, 3), 1, (1,1)),\n",
    "            nn.BatchNorm2d(FM//2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            Conv2d_cd(FM//2, FM//4, (3, 3), 1, (1,1)),\n",
    "            nn.BatchNorm2d(FM//4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.final_patch_size = patchsize//2//2//2\n",
    "        self.out1 =  nn.Linear(self.final_patch_size * self.final_patch_size * (FM//4), Classes)\n",
    "#         self.out1 =  nn.Linear(19200, Classes)\n",
    "        \n",
    "    def forward(self, x1):\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = self.conv3(x1)\n",
    "        x1 = x1.reshape(x1.shape[0], -1)  # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        out1 = self.out1(x1)\n",
    "        return out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f3ab1c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T11:02:51.678206Z",
     "start_time": "2021-11-19T11:02:34.777529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current patch size =  11\n",
      "Current training ratio =  0.1\n",
      "Current dataset =  Houston\n",
      "(359, 1915, 144)\n",
      "Train data shape =  torch.Size([1502, 144, 11, 11])\n",
      "Train label shape =  torch.Size([1502])\n",
      "Test data shape =  torch.Size([13527, 144, 11, 11])\n",
      "Test label shape =  torch.Size([13527])\n",
      "------------------------------------------------------------------------------------------\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 72, 5, 5]            --\n",
      "|    └─Conv2d_cd: 2-1                    [-1, 72, 11, 11]          --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 72, 11, 11]          93,312\n",
      "|    └─BatchNorm2d: 2-2                  [-1, 72, 11, 11]          144\n",
      "|    └─ReLU: 2-3                         [-1, 72, 11, 11]          --\n",
      "|    └─MaxPool2d: 2-4                    [-1, 72, 5, 5]            --\n",
      "├─Sequential: 1-2                        [-1, 36, 2, 2]            --\n",
      "|    └─Conv2d_cd: 2-5                    [-1, 36, 5, 5]            --\n",
      "|    |    └─Conv2d: 3-2                  [-1, 36, 5, 5]            23,328\n",
      "|    └─BatchNorm2d: 2-6                  [-1, 36, 5, 5]            72\n",
      "|    └─ReLU: 2-7                         [-1, 36, 5, 5]            --\n",
      "|    └─MaxPool2d: 2-8                    [-1, 36, 2, 2]            --\n",
      "├─Sequential: 1-3                        [-1, 18, 1, 1]            --\n",
      "|    └─Conv2d_cd: 2-9                    [-1, 18, 2, 2]            --\n",
      "|    |    └─Conv2d: 3-3                  [-1, 18, 2, 2]            5,832\n",
      "|    └─BatchNorm2d: 2-10                 [-1, 18, 2, 2]            36\n",
      "|    └─ReLU: 2-11                        [-1, 18, 2, 2]            --\n",
      "|    └─MaxPool2d: 2-12                   [-1, 18, 1, 1]            --\n",
      "├─Linear: 1-4                            [-1, 15]                  285\n",
      "==========================================================================================\n",
      "Total params: 123,009\n",
      "Trainable params: 123,009\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.07\n",
      "Forward/backward pass size (MB): 0.15\n",
      "Params size (MB): 0.47\n",
      "Estimated Total Size (MB): 0.68\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch:  0 | train loss: 2.1384 | test accuracy: 0.22\n",
      "Epoch:  1 | train loss: 1.7112 | test accuracy: 0.46\n",
      "Time taken to train =  0.7439541816711426 s\n",
      "--------Houston Training Finished-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/jgecvision/.conda/envs/purb37/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type CNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/users/jgecvision/.conda/envs/purb37/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Conv2d_cd. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# CONFIGS\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "datasetNames = [\"Houston\"]\n",
    "testSizeNumber = 2500\n",
    "batchsize = 64\n",
    "EPOCH = 2\n",
    "LR = 0.001\n",
    "trainRatios = [0.1]\n",
    "patchSizes = [11]\n",
    "NUM_ITERATIONS = 1\n",
    "\n",
    "\n",
    "\n",
    "def AA_andEachClassAccuracy(confusion_matrix):\n",
    "    counter = confusion_matrix.shape[0]\n",
    "    list_diag = np.diag(confusion_matrix)\n",
    "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
    "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
    "    average_acc = np.mean(each_acc)\n",
    "    return each_acc, average_acc\n",
    "\n",
    "def reports (xtest,ytest,name):\n",
    "    pred_y = np.empty((len(ytest)), dtype=np.float32)\n",
    "    number = len(ytest) // testSizeNumber\n",
    "    for i in range(number):\n",
    "        temp = xtest[i * testSizeNumber:(i + 1) * testSizeNumber, :, :, :]\n",
    "        temp = temp.cuda()\n",
    "\n",
    "        temp2 = cnn(temp)\n",
    "        temp3 = torch.max(temp2, 1)[1].squeeze()\n",
    "        pred_y[i * testSizeNumber:(i + 1) * testSizeNumber] = temp3.cpu()\n",
    "        del temp, temp2, temp3\n",
    "\n",
    "    if (i + 1) * testSizeNumber < len(ytest):\n",
    "        temp = xtest[(i + 1) * testSizeNumber:len(ytest), :, :, :]\n",
    "        temp = temp.cuda()\n",
    "\n",
    "\n",
    "        temp2 = cnn(temp)\n",
    "        temp3 = torch.max(temp2, 1)[1].squeeze()\n",
    "        pred_y[(i + 1) * testSizeNumber:len(ytest)] = temp3.cpu()\n",
    "        del temp, temp2, temp3\n",
    "\n",
    "    pred_y = torch.from_numpy(pred_y).long()\n",
    "    \n",
    "    if name == 'Houston':\n",
    "        target_names = ['Healthy grass', 'Stressed grass', 'Synthetic grass'\n",
    "                        ,'Trees', 'Soil', 'Water', \n",
    "                        'Residential', 'Commercial', 'Road', 'Highway',\n",
    "                        'Railway', 'Parking Lot 1', 'Parking Lot 2', 'Tennis Court',\n",
    "                        'Running Track']\n",
    "    elif name == 'Trento':\n",
    "        target_names = ['Apples','Buildings','Ground','Woods','Vineyard',\n",
    "                        'Roads']\n",
    "    elif name == 'MUUFL':\n",
    "        target_names = ['Trees','Grass_Pure','Grass_Groundsurface','Dirt_And_Sand', 'Road_Materials','Water',\"Buildings'_Shadow\",\n",
    "                    'Buildings','Sidewalk','Yellow_Curb','ClothPanels']\n",
    "    elif name == 'IP':\n",
    "        target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
    "                ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed', \n",
    "                'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
    "                'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
    "                'Stone-Steel-Towers']\n",
    "    elif name == 'SA':\n",
    "        target_names = ['Brocoli_green_weeds_1','Brocoli_green_weeds_2','Fallow','Fallow_rough_plow','Fallow_smooth',\n",
    "                        'Stubble','Celery','Grapes_untrained','Soil_vinyard_develop','Corn_senesced_green_weeds',\n",
    "                        'Lettuce_romaine_4wk','Lettuce_romaine_5wk','Lettuce_romaine_6wk','Lettuce_romaine_7wk',\n",
    "                        'Vinyard_untrained','Vinyard_vertical_trellis']\n",
    "    elif name == 'UP':\n",
    "        target_names = ['Asphalt','Meadows','Gravel','Trees', 'Painted metal sheets','Bare Soil','Bitumen',\n",
    "                        'Self-Blocking Bricks','Shadows']\n",
    "\n",
    "    \n",
    "    oa = accuracy_score(ytest, pred_y)\n",
    "    confusion = confusion_matrix(ytest, pred_y)\n",
    "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
    "    kappa = cohen_kappa_score(ytest, pred_y)\n",
    "\n",
    "    return confusion, oa*100, each_acc*100, aa*100, kappa*100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for patchsize in patchSizes:\n",
    "    for trainRatio in trainRatios:\n",
    "        for datasetName in datasetNames:\n",
    "            print(\"Current patch size = \",patchsize)\n",
    "            print(\"Current training ratio = \",trainRatio)\n",
    "            print(\"Current dataset = \",datasetName)\n",
    "            \n",
    "            try:\n",
    "                os.makedirs(datasetName)\n",
    "            except FileExistsError:\n",
    "                pass\n",
    "            \n",
    "            X1, _, y = loadData(datasetName)\n",
    "            X1 = normalizeHSI(X1)\n",
    "\n",
    "            X1, yPatch = createImageCubes(X1, y, windowSize=patchsize)\n",
    "            TrainPatch, TestPatch, TrainLabel, TestLabel = splitTrainTestSet(X1, yPatch, 1. - trainRatio, randomState= 42)\n",
    "            TrainPatch = TrainPatch.astype(np.float32)\n",
    "            TestPatch = TestPatch.astype(np.float32)\n",
    "            NC = TrainPatch.shape[3]\n",
    "\n",
    "            TrainPatch = torch.from_numpy(TrainPatch)\n",
    "            TrainLabel = torch.from_numpy(TrainLabel)-1\n",
    "            TrainLabel = TrainLabel.long()\n",
    "            TestPatch = torch.from_numpy(TestPatch)\n",
    "            TestLabel = torch.from_numpy(TestLabel)-1\n",
    "            TestLabel = TestLabel.long()\n",
    "\n",
    "            Classes = len(np.unique(TrainLabel))\n",
    "            TrainPatch = TrainPatch.permute(0,3,1,2)\n",
    "            TestPatch = TestPatch.permute(0,3,1,2)\n",
    "            dataset = dataf.TensorDataset(TrainPatch, TrainLabel)\n",
    "            train_loader = dataf.DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "            \n",
    "            print(\"Train data shape = \", TrainPatch.shape)\n",
    "            print(\"Train label shape = \", TrainLabel.shape)\n",
    "            print(\"Test data shape = \", TestPatch.shape)\n",
    "            print(\"Test label shape = \", TestLabel.shape)\n",
    "\n",
    "            KAPPA = []\n",
    "            OA = []\n",
    "            AA = []\n",
    "            ELEMENT_ACC = np.zeros((NUM_ITERATIONS, Classes))\n",
    "\n",
    "            for iterNum in range(NUM_ITERATIONS):    \n",
    "                cnn = CNN(NC//2, Classes, patchsize,NC)\n",
    "                cnn = cnn.cuda()\n",
    "                summary(cnn, (NC, patchsize, patchsize))\n",
    "                optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "                loss_func = nn.CrossEntropyLoss()  # the target label is not one-hotted\n",
    "\n",
    "                BestAcc = 0\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.time()\n",
    "\n",
    "                # train and test the designed model\n",
    "                for epoch in range(EPOCH):\n",
    "                    for step, (b_x1,b_y) in enumerate(train_loader):\n",
    "                        # move train data to GPU\n",
    "                        b_x1 = b_x1.cuda()\n",
    "                        b_y = b_y.cuda()\n",
    "\n",
    "                        out1 = cnn(b_x1)\n",
    "                        loss = loss_func(out1, b_y)\n",
    "\n",
    "                        optimizer.zero_grad()  # clear gradients for this training step\n",
    "                        loss.backward()  # backpropagation, compute gradients\n",
    "                        optimizer.step()  # apply gradients\n",
    "\n",
    "                        if step == len(train_loader) - 1:\n",
    "                            cnn.eval()\n",
    "                            pred_y = np.empty((len(TestLabel)), dtype='float32')\n",
    "                            number = len(TestLabel) // testSizeNumber\n",
    "                            for i in range(number):\n",
    "                                temp = TestPatch[i * testSizeNumber:(i + 1) * testSizeNumber, :, :, :]\n",
    "                                temp = temp.cuda()\n",
    "\n",
    "                                temp2 = cnn(temp)\n",
    "                                temp3 = torch.max(temp2, 1)[1].squeeze()\n",
    "                                pred_y[i * testSizeNumber:(i + 1) * testSizeNumber] = temp3.cpu()\n",
    "                                del temp, temp2, temp3\n",
    "\n",
    "\n",
    "                            if (i + 1) * testSizeNumber < len(TestLabel):\n",
    "                                temp = TestPatch[(i + 1) * testSizeNumber:len(TestLabel), :, :, :]\n",
    "                                temp = temp.cuda()\n",
    "\n",
    "                                temp2 = cnn(temp)\n",
    "                                temp3 = torch.max(temp2, 1)[1].squeeze()\n",
    "                                pred_y[(i + 1) * testSizeNumber:len(TestLabel)] = temp3.cpu()\n",
    "                                del temp, temp2, temp3\n",
    "\n",
    "                            pred_y = torch.from_numpy(pred_y).long()\n",
    "                            accuracy = torch.sum(pred_y == TestLabel).type(torch.FloatTensor) / TestLabel.size(0)\n",
    "\n",
    "                            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.cpu().numpy(), '| test accuracy: %.2f' % accuracy)\n",
    "\n",
    "                            # save the parameters in network\n",
    "                            if accuracy > BestAcc:\n",
    "                                BestAcc = accuracy\n",
    "                                torch.save(cnn.state_dict(), datasetName+'/net_params_checkpoint.pkl')\n",
    "                            cnn.train()\n",
    "\n",
    "                torch.cuda.synchronize()\n",
    "                end = time.time()\n",
    "                print(\"Time taken to train = \",end - start, \"s\")\n",
    "                Train_time = end - start\n",
    "\n",
    "                cnn.load_state_dict(torch.load(datasetName+'/net_params_checkpoint.pkl'))\n",
    "                cnn.eval()\n",
    "\n",
    "\n",
    "                confusion, oa, each_acc, aa, kappa = reports(TestPatch,TestLabel,datasetName)\n",
    "                KAPPA.append(kappa)\n",
    "                OA.append(oa)\n",
    "                AA.append(aa)\n",
    "                ELEMENT_ACC[iterNum, :] = each_acc\n",
    "                torch.save(cnn, datasetName+'/best_model_G2C-Conv2D-HSI_Iter'+str(iterNum)+'.pt')\n",
    "            print(\"--------\" + datasetName + \" Training Finished-----------\")\n",
    "            record.record_output(OA, AA, KAPPA, ELEMENT_ACC,'./' + datasetName +'/G2C-Conv2D-HSI_Report_' + datasetName +'.txt')\n",
    "                        \n",
    "                        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
